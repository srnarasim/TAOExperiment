{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srnarasim/TAOExperiment/blob/main/TAOExperiment_fixed_updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a0lBEa5262Y"
      },
      "source": [
        "# TAO Experiment - Text Classification with Test-time Adaptation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoVDClc1d7at",
        "outputId": "29457ced-5989-4bdf-cf18-38295f48a674"
      },
      "source": [
        "# Create a CSV file with sample content\n",
        "\n",
        "import csv\n",
        "\n",
        "data = [\n",
        "    ['Product', 'Product Description', 'Category'],\n",
        "    ['Wireless Bluetooth headphones with noise cancellation', 'Headphones', 'Electronics'],\n",
        "    ['Smartphone with OLED display and 128GB storage', 'Smartphone', 'Electronics'],\n",
        "    ['Gaming laptop with high refresh rate screen', 'Laptop', 'Electronics'],\n",
        "    ['Smart home security camera with night vision', 'Smart Home Device', 'Electronics'],\n",
        "    ['Cotton t-shirt with graphic print design', 'T-shirt', 'Clothing'],\n",
        "    ['Wooden dining table with six matching chairs', 'Dining Table', 'Furniture'],\n",
        "    ['Genuine leather wallet with multiple card slots', 'Wallet', 'Accessories'],\n",
        "    ['Insulated stainless steel water bottle', 'Water Bottle', 'Kitchen']\n",
        "]\n",
        "\n",
        "with open('balanced_data.csv', 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerows(data)\n",
        "\n",
        "!cat balanced_data.csv"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Product,Product Description,Category\r\n",
            "Wireless Bluetooth headphones with noise cancellation,Headphones,Electronics\r\n",
            "Smartphone with OLED display and 128GB storage,Smartphone,Electronics\r\n",
            "Gaming laptop with high refresh rate screen,Laptop,Electronics\r\n",
            "Smart home security camera with night vision,Smart Home Device,Electronics\r\n",
            "Cotton t-shirt with graphic print design,T-shirt,Clothing\r\n",
            "Wooden dining table with six matching chairs,Dining Table,Furniture\r\n",
            "Genuine leather wallet with multiple card slots,Wallet,Accessories\r\n",
            "Insulated stainless steel water bottle,Water Bottle,Kitchen\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7klaKYFvd7au",
        "outputId": "d8a726e9-d95d-45a9-e791-08298175edaf"
      },
      "source": [
        "!pip install datasets transformers accelerate bitsandbytes"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjlJjVqDd7au",
        "outputId": "5b3daabf-9322-4ae4-f895-1f16fb190f5e"
      },
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"balanced_data.csv\")\n",
        "\n",
        "# Load tokenizer (using BERT model)\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Create category mapping\n",
        "category_mapping = {category: idx for idx, category in enumerate(df[\"Category\"].unique())}\n",
        "df[\"Label\"] = df[\"Category\"].map(category_mapping)\n",
        "\n",
        "# Tokenize product descriptions\n",
        "max_length = 128\n",
        "encoded_data = tokenizer(\n",
        "    df[\"Product Description\"].tolist(),\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=max_length,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "# Store tokenized data in DataFrame\n",
        "df[\"input_ids\"] = encoded_data[\"input_ids\"].tolist()\n",
        "df[\"attention_mask\"] = encoded_data[\"attention_mask\"].tolist()\n",
        "\n",
        "print(\"Tokenization completed. DataFrame columns:\", df.columns.tolist())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization completed. DataFrame columns: ['Product', 'Product Description', 'Category', 'Label', 'input_ids', 'attention_mask']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "meX5Vovld7au",
        "outputId": "b59b2633-b5a5-4112-8c6f-c2a0b7b216cc"
      },
      "source": [
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments, default_data_collator\n",
        "\n",
        "# ... (rest of your code) ...\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=5,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",  # Ensure this matches the metric returned by compute_metrics\n",
        "    # ... (rest of your training arguments) ...\n",
        ")\n",
        "\n",
        "# Define compute_metrics function\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    # Calculate and return the accuracy\n",
        "    return {\"accuracy\": (preds == labels).mean()}\n",
        "\n",
        "# Initialize trainer, include compute_metrics\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_test[\"train\"],\n",
        "    eval_dataset=train_test[\"test\"],\n",
        "    compute_metrics=compute_metrics # Pass the function to the Trainer\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "print(\"Starting training...\")\n",
        "trainer.train()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 01:12, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.761240</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.787304</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.665977</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.784607</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.784536</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=10, training_loss=1.2079913139343261, metrics={'train_runtime': 80.5321, 'train_samples_per_second': 0.373, 'train_steps_per_second': 0.124, 'total_flos': 77085393300.0, 'train_loss': 1.2079913139343261, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6ozvL_Jd7au"
      },
      "source": [
        "## Test-time Adaptation for New Categories\n",
        "This section implements test-time adaptation to detect products from new, unseen categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMcJS3lXd7au",
        "outputId": "14ef3803-7dc3-4f21-ee30-9f246a0264e6"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "def predict_product_category(text, entropy_threshold=1.5, confidence_threshold=0.4):\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
        "\n",
        "    # Get model predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Convert to probabilities using softmax\n",
        "        probabilities = F.softmax(logits, dim=-1).cpu().numpy()[0]\n",
        "\n",
        "        # Calculate entropy\n",
        "        entropy = -np.sum(probabilities * np.log(probabilities + 1e-9))\n",
        "\n",
        "        # Calculate confidence\n",
        "        confidence = np.max(probabilities)\n",
        "\n",
        "        # Make prediction\n",
        "        if entropy > entropy_threshold or confidence < confidence_threshold:\n",
        "            print(f\"Product '{text}' might be a NEW category!\")\n",
        "            print(f\"Entropy: {entropy:.3f}, Confidence: {confidence:.3f}\")\n",
        "            return 'New Category'\n",
        "        else:\n",
        "            predicted_idx = np.argmax(probabilities)\n",
        "            category = list(category_mapping.keys())[predicted_idx]\n",
        "            print(f\"Product '{text}' classified as: {category}\")\n",
        "            print(f\"Confidence: {confidence:.3f}, Entropy: {entropy:.3f}\")\n",
        "            return category\n",
        "\n",
        "# Test with various product descriptions\n",
        "test_products = [\n",
        "    # Known categories\n",
        "    \"Wireless gaming headphones with RGB lighting\",\n",
        "    \"Wooden dining table with extendable leaf\",\n",
        "    \"Classic leather wallet with coin pocket\",\n",
        "    \"Cotton polo shirt with embroidered logo\",\n",
        "\n",
        "    # Potentially new categories\n",
        "    \"Smart fitness tracker with heart rate monitor\",\n",
        "    \"Electric scooter with foldable design\",\n",
        "    \"Organic green tea from Japan\",\n",
        "    \"Professional oil painting set with easel\",\n",
        "    \"Garden tools set with pruning shears\",\n",
        "    \"Yoga mat with alignment lines\"\n",
        "]\n",
        "\n",
        "print(\"Testing product classification with test-time adaptation:\\n\")\n",
        "results = []\n",
        "for product in test_products:\n",
        "    category = predict_product_category(product)\n",
        "    results.append({'Product': product, 'Predicted Category': category})\n",
        "    print(\"-\" * 80 + \"\\n\")\n",
        "\n",
        "# Display results in a DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\nSummary of Results:\")\n",
        "print(results_df)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing product classification with test-time adaptation:\n",
            "\n",
            "Product 'Wireless gaming headphones with RGB lighting' might be a NEW category!\n",
            "Entropy: 1.531, Confidence: 0.285\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Product 'Wooden dining table with extendable leaf' might be a NEW category!\n",
            "Entropy: 1.530, Confidence: 0.288\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Product 'Classic leather wallet with coin pocket' might be a NEW category!\n",
            "Entropy: 1.548, Confidence: 0.274\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Product 'Cotton polo shirt with embroidered logo' might be a NEW category!\n",
            "Entropy: 1.543, Confidence: 0.292\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Product 'Smart fitness tracker with heart rate monitor' might be a NEW category!\n",
            "Entropy: 1.532, Confidence: 0.285\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Product 'Electric scooter with foldable design' might be a NEW category!\n",
            "Entropy: 1.546, Confidence: 0.292\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Product 'Organic green tea from Japan' might be a NEW category!\n",
            "Entropy: 1.556, Confidence: 0.272\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Product 'Professional oil painting set with easel' might be a NEW category!\n",
            "Entropy: 1.512, Confidence: 0.297\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Product 'Garden tools set with pruning shears' might be a NEW category!\n",
            "Entropy: 1.543, Confidence: 0.286\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Product 'Yoga mat with alignment lines' might be a NEW category!\n",
            "Entropy: 1.555, Confidence: 0.272\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Summary of Results:\n",
            "                                         Product Predicted Category\n",
            "0   Wireless gaming headphones with RGB lighting       New Category\n",
            "1       Wooden dining table with extendable leaf       New Category\n",
            "2        Classic leather wallet with coin pocket       New Category\n",
            "3        Cotton polo shirt with embroidered logo       New Category\n",
            "4  Smart fitness tracker with heart rate monitor       New Category\n",
            "5          Electric scooter with foldable design       New Category\n",
            "6                   Organic green tea from Japan       New Category\n",
            "7       Professional oil painting set with easel       New Category\n",
            "8           Garden tools set with pruning shears       New Category\n",
            "9                  Yoga mat with alignment lines       New Category\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "VyvrNAXCRgYt",
        "outputId": "76ba9b10-7867-4c39-bb3a-23a2ea197ecb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Create a proper dataset from the tokenized data\n",
        "from datasets import Dataset\n",
        "\n",
        "# Convert lists stored as strings back to actual lists if needed\n",
        "import ast\n",
        "\n",
        "# Create a function to safely convert string representations of lists to actual lists\n",
        "def safe_eval(x):\n",
        "    if isinstance(x, list):\n",
        "        return x\n",
        "    try:\n",
        "        return ast.literal_eval(x)\n",
        "    except (ValueError, SyntaxError):\n",
        "        return x\n",
        "\n",
        "# Prepare the dataset\n",
        "dataset_dict = {\n",
        "    'input_ids': [safe_eval(ids) for ids in df['input_ids']],\n",
        "    'attention_mask': [safe_eval(mask) for mask in df['attention_mask']],\n",
        "    'labels': df['Label'].tolist()\n",
        "}\n",
        "\n",
        "# Create the dataset\n",
        "new_dataset = Dataset.from_dict(dataset_dict)\n",
        "print(f\"Created dataset with {len(new_dataset)} examples\")\n",
        "print(f\"Dataset features: {new_dataset.features}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created dataset with 8 examples\n",
            "Dataset features: {'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'labels': Value(dtype='int64', id=None)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import get_peft_model, LoraConfig\n",
        "from transformers import default_data_collator, Trainer, TrainingArguments\n",
        "\n",
        "# ... (rest of your code) ...\n",
        "\n",
        "# Reinitialize trainer with updated training_args and data_collator, pass compute_metrics to the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=new_dataset,  # Use new_dataset for fine-tuning\n",
        "    eval_dataset=new_dataset,   # Use new_dataset for evaluation as well\n",
        "    data_collator=default_data_collator,  # Ensure proper data collation\n",
        "    compute_metrics=lambda pred: {'accuracy': (pred.predictions.argmax(-1) == pred.label_ids).mean()} # Pass compute_metrics here\n",
        ")\n",
        "\n",
        "# Modify the compute_loss method to accept num_items_in_batch\n",
        "def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None): # Add num_items_in_batch as an argument\n",
        "    \"\"\"\n",
        "    How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
        "    Subclass and override for custom behavior.\n",
        "    \"\"\"\n",
        "    if self.label_smoother is not None and \"labels\" in inputs:\n",
        "        labels = inputs.pop(\"labels\")\n",
        "    else:\n",
        "        labels = None\n",
        "\n",
        "    # Remove num_items_in_batch from inputs if it exists to avoid issues with the model\n",
        "    inputs.pop(\"num_items_in_batch\", None)\n",
        "\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    # Save past state if it exists\n",
        "    # TODO: this needs to be fixed and made cleaner later.\n",
        "    if self.args.past_index >= 0:\n",
        "        self._past_state = outputs[self.args.past_index]\n",
        "\n",
        "    if labels is not None:\n",
        "        loss = self.label_smoother(outputs, labels)\n",
        "    else:\n",
        "        # We don't use .loss here since the model may return tuples instead of ModelOutput.\n",
        "        loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
        "\n",
        "    return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# Assign the modified compute_loss method to the trainer\n",
        "trainer.compute_loss = compute_loss.__get__(trainer) # type: ignore\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "BAJMpvUwBo2T",
        "outputId": "76d522d0-443a-4123-e3a8-e94619f9438a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 01:01, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.313603</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.239264</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.131488</td>\n",
              "      <td>0.625000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.065731</td>\n",
              "      <td>0.625000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.040174</td>\n",
              "      <td>0.625000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=10, training_loss=1.2382075309753418, metrics={'train_runtime': 63.5946, 'train_samples_per_second': 0.629, 'train_steps_per_second': 0.157, 'total_flos': 102780524400.0, 'train_loss': 1.2382075309753418, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "def predict_product_category(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    probabilities = F.softmax(outputs.logits, dim=-1).cpu().numpy()[0]\n",
        "    entropy = -np.sum(probabilities * np.log(probabilities + 1e-9))  # Compute entropy\n",
        "\n",
        "    if entropy > 1.5:  # Threshold for uncertainty\n",
        "        print(f\"Product '{text}' is a potential NEW category!\")\n",
        "        return \"New Category\"\n",
        "    else:\n",
        "        predicted_label = np.argmax(probabilities)\n",
        "        category = list(category_mapping.keys())[predicted_label]\n",
        "        return category\n",
        "\n",
        "# Example: New product descriptions\n",
        "new_products = [\n",
        "    \"Smart fitness tracker with heart rate monitor\",\n",
        "    \"Electric scooter with foldable design\"\n",
        "]\n",
        "\n",
        "for product in new_products:\n",
        "    print(predict_product_category(product))\n"
      ],
      "metadata": {
        "id": "zVRMUmIdSkBz",
        "outputId": "5c890967-744d-40d0-bba1-859fbec34e81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Product 'Smart fitness tracker with heart rate monitor' is a potential NEW category!\n",
            "New Category\n",
            "Product 'Electric scooter with foldable design' is a potential NEW category!\n",
            "New Category\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import get_peft_model, LoraConfig\n",
        "\n",
        "# LoRA configuration\n",
        "lora_config = LoraConfig(r=8, lora_alpha=16, lora_dropout=0.1)\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Fine-tune only on new categories\n",
        "new_product_data = [\n",
        "    {\"text\": \"Smart fitness tracker with heart rate monitor\", \"label\": len(category_mapping)},\n",
        "    {\"text\": \"Electric scooter with foldable design\", \"label\": len(category_mapping) + 1}\n",
        "]\n",
        "new_df = pd.DataFrame(new_product_data)\n",
        "new_dataset = Dataset.from_pandas(new_df)\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "ppoDMzymSuPX",
        "outputId": "62166f5f-26bd-435e-b2d8-40956f623133",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 00:47, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.131488</td>\n",
              "      <td>0.625000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.131488</td>\n",
              "      <td>0.625000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.131488</td>\n",
              "      <td>0.625000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.131488</td>\n",
              "      <td>0.625000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.131488</td>\n",
              "      <td>0.625000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=10, training_loss=1.1479059219360352, metrics={'train_runtime': 48.5092, 'train_samples_per_second': 0.825, 'train_steps_per_second': 0.206, 'total_flos': 103134418800.0, 'train_loss': 1.1479059219360352, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}