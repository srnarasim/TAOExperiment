{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/srnarasim/TAOExperiment/blob/main/TAOExperiment_LLM_LoRA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAO Experiment - Text Classification with LLM and LoRA\n",
    "\n",
    "This notebook implements Test-time Adaptation for Out-of-distribution detection (TAO) using:\n",
    "1. Fine-tuning a pre-trained LLM (Llama-2-7b) on known product categories\n",
    "2. Implementing test-time adaptation to dynamically handle new categories\n",
    "3. Using LoRA for lightweight updates when new categories emerge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets accelerate bitsandbytes peft trl torch pandas numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, PeftConfig\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Sample Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CSV file with sample content\n",
    "import csv\n",
    "\n",
    "data = [\n",
    "    ['Product', 'Product Description', 'Category'],\n",
    "    ['Wireless Bluetooth headphones with noise cancellation', 'Headphones', 'Electronics'],\n",
    "    ['Smartphone with OLED display and 128GB storage', 'Smartphone', 'Electronics'],\n",
    "    ['Gaming laptop with high refresh rate screen', 'Laptop', 'Electronics'],\n",
    "    ['Smart home security camera with night vision', 'Smart Home Device', 'Electronics'],\n",
    "    ['Cotton t-shirt with graphic print design', 'T-shirt', 'Clothing'],\n",
    "    ['Denim jeans with distressed finish', 'Jeans', 'Clothing'],\n",
    "    ['Wool sweater with cable knit pattern', 'Sweater', 'Clothing'],\n",
    "    ['Leather jacket with quilted lining', 'Jacket', 'Clothing'],\n",
    "    ['Wooden dining table with six matching chairs', 'Dining Table', 'Furniture'],\n",
    "    ['Memory foam mattress with cooling gel', 'Mattress', 'Furniture'],\n",
    "    ['Bookshelf with adjustable shelves', 'Bookshelf', 'Furniture'],\n",
    "    ['Leather sofa with reclining function', 'Sofa', 'Furniture'],\n",
    "    ['Genuine leather wallet with multiple card slots', 'Wallet', 'Accessories'],\n",
    "    ['Stainless steel watch with leather strap', 'Watch', 'Accessories'],\n",
    "    ['Polarized sunglasses with UV protection', 'Sunglasses', 'Accessories'],\n",
    "    ['Silk scarf with floral pattern', 'Scarf', 'Accessories'],\n",
    "    ['Insulated stainless steel water bottle', 'Water Bottle', 'Kitchen'],\n",
    "    ['Non-stick frying pan with glass lid', 'Frying Pan', 'Kitchen'],\n",
    "    ['Electric coffee maker with programmable timer', 'Coffee Maker', 'Kitchen'],\n",
    "    ['Ceramic dinner plate set with modern design', 'Plate Set', 'Kitchen']\n",
    "]\n",
    "\n",
    "with open('balanced_data.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerows(data)\n",
    "\n",
    "# Display the dataset\n",
    "df = pd.read_csv('balanced_data.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Categories: {df['Category'].unique()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Data for LLM Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format data for instruction fine-tuning\n",
    "def format_instruction(product):\n",
    "    return f\"\"\"### Instruction: \n",
    "Classify the following product into one of these categories: Electronics, Clothing, Furniture, Accessories, Kitchen.\n",
    "\n",
    "### Product:\n",
    "{product}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "# Prepare dataset\n",
    "df = df.iloc[1:].reset_index(drop=True)  # Remove header row if it was included\n",
    "df['instruction'] = df['Product'].apply(format_instruction)\n",
    "df['response'] = df['Category']\n",
    "\n",
    "# Split into train and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['Category'], random_state=42)\n",
    "\n",
    "# Convert to HuggingFace datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Testing samples: {len(test_dataset)}\")\n",
    "\n",
    "# Display a sample\n",
    "print(\"\\nSample instruction:\")\n",
    "print(train_dataset[0]['instruction'])\n",
    "print(f\"Expected response: {train_dataset[0]['response']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Pre-trained LLM and Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model name\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"  # You can also use \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" for faster training\n",
    "\n",
    "# Configure quantization for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"right\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Rank\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"Model loaded with LoRA configuration\")\n",
    "print(model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fine-tune the Model on Known Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/llm-product-classifier\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    save_steps=50,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Initialize SFT trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=lora_config,\n",
    "    dataset_text_field=\"instruction\",\n",
    "    max_seq_length=512\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./results/llm-product-classifier-final\")\n",
    "tokenizer.save_pretrained(\"./results/llm-product-classifier-final\")\n",
    "print(\"Model training completed and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate the Model on Known Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model\n",
    "peft_model_path = \"./results/llm-product-classifier-final\"\n",
    "config = PeftConfig.from_pretrained(peft_model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, peft_model_path)\n",
    "\n",
    "# Create a text generation pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=20,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# Function to predict category\n",
    "def predict_category(product):\n",
    "    prompt = format_instruction(product)\n",
    "    response = pipe(prompt)[0]['generated_text']\n",
    "    # Extract the category from the response\n",
    "    category = response.split(\"### Response:\\n\")[-1].strip()\n",
    "    return category\n",
    "\n",
    "# Evaluate on test set\n",
    "true_categories = []\n",
    "predicted_categories = []\n",
    "\n",
    "for i, sample in enumerate(test_dataset):\n",
    "    product = sample['Product']\n",
    "    true_category = sample['Category']\n",
    "    predicted_category = predict_category(product)\n",
    "    \n",
    "    true_categories.append(true_category)\n",
    "    predicted_categories.append(predicted_category)\n",
    "    \n",
    "    print(f\"Product: {product}\")\n",
    "    print(f\"True: {true_category}, Predicted: {predicted_category}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_categories, predicted_categories)\n",
    "print(f\"\\nAccuracy on known categories: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_categories, predicted_categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Implement Test-time Adaptation (TAO) for New Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Function to calculate entropy from model outputs\n",
    "def calculate_entropy(logits):\n",
    "    # Get probabilities using softmax\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    # Calculate entropy\n",
    "    entropy = -torch.sum(probs * torch.log(probs + 1e-9), dim=-1)\n",
    "    return entropy.mean().item()\n",
    "\n",
    "# Function to predict with uncertainty estimation\n",
    "def predict_with_tao(product, entropy_threshold=5.0):\n",
    "    prompt = format_instruction(product)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Calculate entropy on the last token's logits\n",
    "        last_token_logits = logits[:, -1, :]\n",
    "        entropy = calculate_entropy(last_token_logits)\n",
    "        \n",
    "        # Generate response\n",
    "        response = pipe(prompt)[0]['generated_text']\n",
    "        category = response.split(\"### Response:\\n\")[-1].strip()\n",
    "        \n",
    "        # Check if entropy is high (uncertain prediction)\n",
    "        if entropy > entropy_threshold:\n",
    "            return \"New Category\", entropy\n",
    "        else:\n",
    "            return category, entropy\n",
    "\n",
    "# Test with known and new products\n",
    "test_products = [\n",
    "    # Known categories\n",
    "    \"Wireless gaming headphones with RGB lighting\",\n",
    "    \"Wooden dining table with extendable leaf\",\n",
    "    \"Classic leather wallet with coin pocket\",\n",
    "    \"Cotton polo shirt with embroidered logo\",\n",
    "    \n",
    "    # Potentially new categories\n",
    "    \"Smart fitness tracker with heart rate monitor\",  # Could be new (Wearable Tech)\n",
    "    \"Electric scooter with foldable design\",          # Could be new (Transportation)\n",
    "    \"Organic vitamin supplements in glass bottle\",    # New (Health & Wellness)\n",
    "    \"Handcrafted ceramic plant pot with drainage\"     # New (Home & Garden)\n",
    "]\n",
    "\n",
    "print(\"Testing product classification with test-time adaptation:\\n\")\n",
    "for product in test_products:\n",
    "    category, entropy = predict_with_tao(product)\n",
    "    print(f\"Product: '{product}'\")\n",
    "    print(f\"Classification: {category}\")\n",
    "    print(f\"Uncertainty (Entropy): {entropy:.4f}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. LoRA Adaptation for New Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with new categories\n",
    "new_categories_data = [\n",
    "    ['Product', 'Product Description', 'Category'],\n",
    "    ['Smart fitness tracker with heart rate monitor', 'Fitness Tracker', 'Wearable Tech'],\n",
    "    ['Smartwatch with GPS and sleep tracking', 'Smartwatch', 'Wearable Tech'],\n",
    "    ['Wireless earbuds with fitness tracking', 'Earbuds', 'Wearable Tech'],\n",
    "    ['Health monitoring ring with sleep analysis', 'Smart Ring', 'Wearable Tech'],\n",
    "    ['Electric scooter with foldable design', 'Electric Scooter', 'Transportation'],\n",
    "    ['Electric bicycle with pedal assist', 'E-Bike', 'Transportation'],\n",
    "    ['Hoverboard with LED lights', 'Hoverboard', 'Transportation'],\n",
    "    ['Electric skateboard with remote control', 'E-Skateboard', 'Transportation'],\n",
    "    ['Organic vitamin supplements in glass bottle', 'Vitamins', 'Health & Wellness'],\n",
    "    ['Essential oil diffuser with LED lights', 'Diffuser', 'Health & Wellness'],\n",
    "    ['Yoga mat with alignment markings', 'Yoga Mat', 'Health & Wellness'],\n",
    "    ['Meditation cushion with organic filling', 'Meditation Cushion', 'Health & Wellness'],\n",
    "    ['Handcrafted ceramic plant pot with drainage', 'Plant Pot', 'Home & Garden'],\n",
    "    ['Indoor herb garden kit with grow light', 'Herb Garden', 'Home & Garden'],\n",
    "    ['Gardening tool set with ergonomic handles', 'Garden Tools', 'Home & Garden'],\n",
    "    ['Self-watering planter for indoor plants', 'Self-watering Planter', 'Home & Garden']\n",
    "]\n",
    "\n",
    "with open('new_categories_data.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerows(new_categories_data)\n",
    "\n",
    "# Load and prepare the new categories dataset\n",
    "new_df = pd.read_csv('new_categories_data.csv')\n",
    "new_df = new_df.iloc[1:].reset_index(drop=True)  # Remove header row\n",
    "new_df['instruction'] = new_df['Product'].apply(format_instruction)\n",
    "new_df['response'] = new_df['Category']\n",
    "\n",
    "# Convert to HuggingFace dataset\n",
    "new_categories_dataset = Dataset.from_pandas(new_df)\n",
    "print(f\"New categories dataset size: {len(new_categories_dataset)}\")\n",
    "print(f\"New categories: {new_df['Category'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new LoRA adapter for the new categories\n",
    "new_lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    # Use a different adapter name for the new categories\n",
    "    modules_to_save=[\"lm_head\"]\n",
    ")\n",
    "\n",
    "# Apply the new LoRA adapter to the model\n",
    "model_for_new_categories = get_peft_model(model, new_lora_config)\n",
    "print(\"Model prepared with new LoRA adapter for new categories\")\n",
    "print(model_for_new_categories.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments for the new categories\n",
    "new_training_args = TrainingArguments(\n",
    "    output_dir=\"./results/llm-new-categories\",\n",
    "    num_train_epochs=5,  # More epochs for better learning of new categories\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    save_steps=20,\n",
    "    logging_steps=5,\n",
    "    learning_rate=5e-4,  # Higher learning rate for faster adaptation\n",
    "    weight_decay=0.001,\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Initialize SFT trainer for new categories\n",
    "new_trainer = SFTTrainer(\n",
    "    model=model_for_new_categories,\n",
    "    train_dataset=new_categories_dataset,\n",
    "    args=new_training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=new_lora_config,\n",
    "    dataset_text_field=\"instruction\",\n",
    "    max_seq_length=512\n",
    ")\n",
    "\n",
    "# Train the model on new categories\n",
    "print(\"Starting training on new categories...\")\n",
    "new_trainer.train()\n",
    "\n",
    "# Save the fine-tuned model for new categories\n",
    "model_for_new_categories.save_pretrained(\"./results/llm-new-categories-final\")\n",
    "tokenizer.save_pretrained(\"./results/llm-new-categories-final\")\n",
    "print(\"Model training on new categories completed and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test the Adapted Model on Both Known and New Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with the new LoRA adapter\n",
    "new_peft_model_path = \"./results/llm-new-categories-final\"\n",
    "new_config = PeftConfig.from_pretrained(new_peft_model_path)\n",
    "adapted_model = AutoModelForCausalLM.from_pretrained(\n",
    "    new_config.base_model_name_or_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "adapted_model = PeftModel.from_pretrained(adapted_model, new_peft_model_path)\n",
    "\n",
    "# Create a text generation pipeline with the adapted model\n",
    "adapted_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=adapted_model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=20,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# Function to predict category with the adapted model\n",
    "def predict_with_adapted_model(product):\n",
    "    prompt = format_instruction(product)\n",
    "    response = adapted_pipe(prompt)[0]['generated_text']\n",
    "    category = response.split(\"### Response:\\n\")[-1].strip()\n",
    "    return category\n",
    "\n",
    "# Test with both known and new categories\n",
    "combined_test_products = [\n",
    "    # Known categories\n",
    "    \"Wireless gaming headphones with RGB lighting\",\n",
    "    \"Wooden dining table with extendable leaf\",\n",
    "    \"Classic leather wallet with coin pocket\",\n",
    "    \"Cotton polo shirt with embroidered logo\",\n",
    "    \n",
    "    # New categories\n",
    "    \"Smart fitness tracker with heart rate monitor\",\n",
    "    \"Electric scooter with foldable design\",\n",
    "    \"Organic vitamin supplements in glass bottle\",\n",
    "    \"Handcrafted ceramic plant pot with drainage\",\n",
    "    \n",
    "    # Additional test cases\n",
    "    \"Bluetooth smartwatch with fitness tracking\",\n",
    "    \"Electric unicycle with self-balancing technology\",\n",
    "    \"Meditation app subscription with guided sessions\",\n",
    "    \"Automatic plant watering system with soil sensors\"\n",
    "]\n",
    "\n",
    "print(\"Testing the adapted model on both known and new categories:\\n\")\n",
    "for product in combined_test_products:\n",
    "    category = predict_with_adapted_model(product)\n",
    "    print(f\"Product: '{product}'\")\n",
    "    print(f\"Classification: {category}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Implement Continuous Test-time Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to simulate continuous test-time adaptation\n",
    "def continuous_tao(product, known_categories, entropy_threshold=5.0, confidence_threshold=0.7):\n",
    "    prompt = format_instruction(product)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Calculate entropy on the last token's logits\n",
    "        last_token_logits = logits[:, -1, :]\n",
    "        entropy = calculate_entropy(last_token_logits)\n",
    "        \n",
    "        # Generate response\n",
    "        response = adapted_pipe(prompt)[0]['generated_text']\n",
    "        category = response.split(\"### Response:\\n\")[-1].strip()\n",
    "        \n",
    "        # Check if the predicted category is in known categories\n",
    "        if category in known_categories:\n",
    "            confidence = 1.0 - (entropy / 10.0)  # Normalize entropy to a confidence score\n",
    "            if confidence < confidence_threshold:\n",
    "                return \"Uncertain - Needs Human Review\", entropy, confidence\n",
    "            else:\n",
    "                return category, entropy, confidence\n",
    "        else:\n",
    "            # This is a new category prediction\n",
    "            if entropy > entropy_threshold:\n",
    "                return \"New Category - High Uncertainty\", entropy, 0.0\n",
    "            else:\n",
    "                return category, entropy, 0.8  # Reasonable confidence in the new category\n",
    "\n",
    "# List of known categories\n",
    "known_categories = ['Electronics', 'Clothing', 'Furniture', 'Accessories', 'Kitchen', \n",
    "                    'Wearable Tech', 'Transportation', 'Health & Wellness', 'Home & Garden']\n",
    "\n",
    "# Test products for continuous adaptation\n",
    "continuous_test_products = [\n",
    "    # Known categories with clear classification\n",
    "    \"High-end gaming laptop with RTX graphics\",\n",
    "    \"Merino wool sweater with turtleneck design\",\n",
    "    \n",
    "    # Known categories but unusual descriptions\n",
    "    \"Foldable smartphone with flexible OLED screen\",\n",
    "    \"Modular sofa that transforms into a bed\",\n",
    "    \n",
    "    # New categories from our adaptation\n",
    "    \"GPS running watch with heart rate monitoring\",\n",
    "    \"Collapsible electric scooter with removable battery\",\n",
    "    \n",
    "    # Potentially completely new categories\n",
    "    \"NFT digital art collection with blockchain certificate\",\n",
    "    \"Virtual reality meditation experience subscription\",\n",
    "    \"AI-powered personal shopping assistant service\",\n",
    "    \"Biodegradable phone case made from plant materials\"\n",
    "]\n",
    "\n",
    "print(\"Testing continuous test-time adaptation:\\n\")\n",
    "for product in continuous_test_products:\n",
    "    category, entropy, confidence = continuous_tao(product, known_categories)\n",
    "    print(f\"Product: '{product}'\")\n",
    "    print(f\"Classification: {category}\")\n",
    "    print(f\"Uncertainty (Entropy): {entropy:.4f}, Confidence: {confidence:.4f}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusion and Next Steps\n",
    "\n",
    "In this notebook, we've implemented a Test-time Adaptation for Out-of-distribution detection (TAO) system using:\n",
    "\n",
    "1. A pre-trained LLM (Llama-2) fine-tuned on known product categories\n",
    "2. LoRA for efficient adaptation to new categories\n",
    "3. Entropy-based uncertainty estimation for detecting out-of-distribution samples\n",
    "4. Continuous test-time adaptation for handling evolving product categories\n",
    "\n",
    "Next steps for improving this system could include:\n",
    "\n",
    "- Implementing active learning to selectively query human experts for uncertain predictions\n",
    "- Developing a more sophisticated uncertainty estimation method beyond entropy\n",
    "- Creating a feedback loop for continuous model improvement\n",
    "- Implementing a multi-adapter approach to handle different domains separately\n",
    "- Exploring parameter-efficient fine-tuning methods beyond LoRA (e.g., QLoRA, IAÂ³)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}